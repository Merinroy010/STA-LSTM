{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\RWL_prediction\\STA-LSTM\\src\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from data import data_preprocess, data_trans\n",
    "from modelbase import STA_LSTM as Net\n",
    "# from modelbase import SA_LSTM as Net\n",
    "# from modelbase import TA_LSTM as Net\n",
    "# from modelbase import LSTM as Net\n",
    "# from modelbase import FCN as Net\n",
    "# from modelbase import SVM as Net\n",
    "print(os.getcwd())\n",
    "'''****************************initialization*******************************''' \n",
    "IN_DIM =  144       # 因变量 TX144，CH96，HH120\n",
    "SEQUENCE_LENGTH = 12   # 时间序列长度，即为回溯期\n",
    "\n",
    "LSTM_IN_DIM = int(IN_DIM/SEQUENCE_LENGTH)     # LSTM的input大小,等于总的变量长度/时间序列长度\n",
    "LSTM_HIDDEN_DIM = 300  # LSTM隐状态的大小\n",
    "\n",
    "OUT_DIM = 1            # 输出大小\n",
    "\n",
    "LEARNING_RATE = 0.05 # learning rate\n",
    "WEIGHT_DECAY = 1e-6    # L2惩罚项\n",
    "\n",
    "BATCH_SIZE = 200        # batch size\n",
    "\n",
    "EPOCHES = 180    # epoch大小\n",
    "\n",
    "TRAIN_PER = 0.80 # 训练集占比\n",
    "VALI_PER = 0.0 # 验证集占比\n",
    "\n",
    "# 判断是否采用GPU加速\n",
    "# USE_GPU = torch.cuda.is_available()\n",
    "USE_GPU = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''****************************data prepration*******************************''' \n",
    "# 准备好训练和测试数据\n",
    "dp = data_preprocess(file_path = '../data/dataset/sample_t+1.csv', train_per = TRAIN_PER, vali_per = VALI_PER, in_dim = IN_DIM)\n",
    "\n",
    "raw_data = dp.load_data()\n",
    "# print('数据导入完成')\n",
    "\n",
    "(train_data,train_groundtruth),(vali_data,vali_groundtruth),(test_data,test_groundtruth) = dp.split_data(raw_data = raw_data, _type = 'linear')\n",
    "# print('数据分割完成')\n",
    "\n",
    "# 设置对数据进行的转换方式，transform.compose的作用是将多个transform组合到一起进行使用\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=(0,0,0),std=(1,1,1))])\n",
    "# print('数据转换为tensor')\n",
    "\n",
    "# data_trans返回的值是一个字典，内部包含数据和真值{'inputs':inputs,'groundtruth':groundtruths}\n",
    "\n",
    "# 准备训练集\n",
    "train_data_trans = data_trans(train_data,train_groundtruth,transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data_trans,\n",
    "                                           batch_size =BATCH_SIZE,\n",
    "                                           shuffle = True,\n",
    "                                           num_workers = 4)\n",
    "# print('训练集准备完毕')\n",
    "\n",
    "# 准备测试集\n",
    "test_data_trans = data_trans(test_data, test_groundtruth,transform)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data_trans,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = False,\n",
    "                                           num_workers = 4)\n",
    "# print('测试集准备完毕')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''****************************model prepration*******************************''' \n",
    "# 将网络参数导入网络\n",
    "net = Net(IN_DIM,SEQUENCE_LENGTH,LSTM_IN_DIM,LSTM_HIDDEN_DIM,OUT_DIM,USE_GPU)\n",
    "# print('网络模型准备完毕')\n",
    "\n",
    "# 判断GPU是否可用，如果可用则将net变成可用GPU加速的net\n",
    "if USE_GPU:\n",
    "    net = net.cuda()\n",
    "    # print('本次实验使用GPU加速')\n",
    "else:\n",
    "    pass\n",
    "    # print('本次实验不使用GPU加速')\n",
    "\n",
    "# 使用SGD（随机梯度下降）优化，学习率为0.001，动量为0.9\n",
    "# optimizer = optim.SGD(net.parameters(), lr= LEARNING_RATE, momentum=0.9) \n",
    "# 根据梯度调整参数数值，Adam算法\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# 学习率根据训练的次数进行调整\n",
    "adjust_lr = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                     milestones=[i*10 for i in range(EPOCHES//10)],\n",
    "                                     gamma=0.5)\n",
    "\n",
    "# 定义训练损失函数&测试误差函数\n",
    "# loss_criterion = nn.SmoothL1Loss()\n",
    "loss_criterion = nn.MSELoss()\n",
    "error_criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(verbose = False):\n",
    "\n",
    "    net.train()\n",
    "    loss_list = []\n",
    "\n",
    "    for i,data in enumerate(train_dataloader):\n",
    "       \n",
    "        inputs = data['inputs']\n",
    "        groundtruths = data['groundtruths']     \n",
    "        \n",
    "        if USE_GPU:\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            groundtruths = Variable(groundtruths).cuda()\n",
    "            \n",
    "        else:\n",
    "            inputs = Variable(inputs)\n",
    "            groundtruths = Variable(groundtruths)\n",
    "        \n",
    "        #将参数的grad值初始化为0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #获得网络输出结果\n",
    "        out = net(inputs)\n",
    "\n",
    "        #根据真值计算损失函数的值\n",
    "        loss = loss_criterion(out,groundtruths)\n",
    "\n",
    "        #通过优化器优化网络\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "      \n",
    "    return loss_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    \n",
    "    error = 0.0\n",
    "    predictions = []\n",
    "    test_groundtruths = []\n",
    "\n",
    "    # 告诉网络进行测试，不再是训练模式\n",
    "    net.eval() \n",
    "\n",
    "    for i,data in enumerate(test_dataloader):\n",
    "\n",
    "        inputs = data['inputs']\n",
    "        groundtruths = data['groundtruths']     \n",
    "        \n",
    "        if USE_GPU:\n",
    "\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            groundtruths = Variable(groundtruths).cuda()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            inputs = Variable(inputs)\n",
    "            groundtruths = Variable(groundtruths)\n",
    "\n",
    "        out = net(inputs)\n",
    "        error += (error_criterion(out,groundtruths).item()*groundtruths.size(0))\n",
    "\n",
    "        if USE_GPU:\n",
    "            predictions.extend(out.cpu().data.numpy().tolist())\n",
    "            test_groundtruths.extend(groundtruths.cpu().data.numpy().tolist())\n",
    "            \n",
    "        else:\n",
    "            predictions.extend(out.data.numpy().tolist())\n",
    "            test_groundtruths.extend(groundtruths.data.numpy().tolist())\n",
    "      \n",
    "    average_error = np.sqrt(error/len(test_data_trans))\n",
    "    \n",
    "    return np.array(predictions).reshape((len(predictions))),np.array(test_groundtruths).reshape((len(test_groundtruths))),average_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\merin\\miniforge3\\envs\\flood_inundation-env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'STA_LSTM' object has no attribute 'batch_norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(net,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/sta_lstm_t+1.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHES):\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# adjust learning rate\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     adjust_lr\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 14\u001b[0m     loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     loss_recorder\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(loss_list))\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m,loss = \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,np\u001b[38;5;241m.\u001b[39mmean(loss_list)))\n",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(verbose)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#获得网络输出结果\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#根据真值计算损失函数的值\u001b[39;00m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_criterion(out,groundtruths)\n",
      "File \u001b[1;32mc:\\Users\\merin\\miniforge3\\envs\\flood_inundation-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\merin\\miniforge3\\envs\\flood_inundation-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\RWL_prediction\\STA-LSTM\\src\\modelbase.py:222\u001b[0m, in \u001b[0;36mSTA_LSTM.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m (\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    220\u001b[0m \n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# 批归一化处理输入\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# print('batch_norm',out.size())\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# 经过输入层处理\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_in(out)\n",
      "File \u001b[1;32mc:\\Users\\merin\\miniforge3\\envs\\flood_inundation-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'STA_LSTM' object has no attribute 'batch_norm'"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \n",
    "    #记录程序开始的时间\n",
    "    train_start = time.time()\n",
    "    loss_recorder = []\n",
    "    \n",
    "    print('starting training... ')\n",
    "\n",
    "    for epoch in range(EPOCHES):\n",
    "\n",
    "        # adjust learning rate\n",
    "        adjust_lr.step()\n",
    "        \n",
    "        loss_list = train(verbose= True)\n",
    "        \n",
    "        loss_recorder.append(np.mean(loss_list))\n",
    "        \n",
    "        print('epoch = %d,loss = %.5f'%(epoch+1,np.mean(loss_list)))\n",
    "    \n",
    "    print ('training time = {}s'.format(int((time.time() - train_start))))\n",
    "    \n",
    "    # 记录测试开始的时间\n",
    "    test_start = time.time()\n",
    "    predictions, test_groundtruth, average_error = test()\n",
    "\n",
    "    print(predictions.shape)\n",
    "    print(test_groundtruth.shape)\n",
    "    \n",
    "    print('test time = {}s'.format(int((time.time() - test_start)+1.0)))\n",
    "    print('average error = ',  average_error)\n",
    "\n",
    "    result = pd.DataFrame(data = {'Q(t+1)':predictions,'Q(t+1)truth':test_groundtruth})\n",
    "    result.to_csv('./data/output/out_t+1.csv')\n",
    "    \n",
    "    torch.save(net,'./models/sta_lstm_t+1.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood_inundation-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
